import os
import json
import pymongo
from datetime import datetime
from dotenv import load_dotenv

load_dotenv()

def find_data_directory():
    """Trouver automatiquement le bon chemin vers les donn√©es"""
    possible_paths = [
        "data/api-epis_pollution_cleaned",      # Si ex√©cut√© depuis la racine
        "../data/api-epis_pollution_cleaned",   # Si ex√©cut√© depuis scripts/
        "../../data/api-epis_pollution_cleaned" # Au cas o√π
    ]
    
    for path in possible_paths:
        if os.path.exists(path):
            print(f"‚úÖ Dossier trouv√© : {path}")
            return path
    
    print("‚ùå Dossier de donn√©es non trouv√©")
    return None

def import_episodes_to_mongo():
    """Importer les √©pisodes de pollution nettoy√©s dans MongoDB"""
    
    connection_string = os.getenv('MONGO_CONNECTION_STRING', 'mongodb://localhost:27017/')
    database_name = os.getenv('MONGO_DATABASE', 'pollution_app')
    
    print("üö® IMPORT DES √âPISODES DE POLLUTION (VERSION CORRIG√âE)")
    print("=" * 60)
    
    # Trouver le bon chemin
    episodes_dir = find_data_directory()
    if not episodes_dir:
        return
    
    client = pymongo.MongoClient(connection_string)
    db = client[database_name]
    
    # Fichiers √† importer
    episode_files = [
        "cleaned_data_no2.json",
        "cleaned_data_o3.json", 
        "cleaned_data_pm10.json",
        "cleaned_data_pm25.json"
    ]
    
    total_imported = 0
    
    for filename in episode_files:
        filepath = os.path.join(episodes_dir, filename)
        
        if not os.path.exists(filepath):
            print(f"‚ùå Fichier non trouv√© : {filepath}")
            continue
            
        print(f"üìÇ Traitement : {filename}")
        
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # Extraire le polluant du nom de fichier
            polluant = filename.replace('cleaned_data_', '').replace('.json', '').upper()
            
            # Traiter les features
            features = data.get('features', [])
            print(f"   üìä {len(features)} √©pisodes trouv√©s")
            
            # Cr√©er une collection temporaire pour ce polluant
            collection_name = f"episodes_{polluant.lower()}"
            collection = db[collection_name]
            
            # Vider la collection existante
            collection.drop()
            
            # Pr√©parer les documents √† ins√©rer
            documents = []
            for i, feature in enumerate(features):
                properties = feature.get('properties', {})
                geometry = feature.get('geometry', {})
                coordinates = geometry.get('coordinates', [])
                
                doc = {
                    "_id": f"episode_{polluant}_{i}",
                    "type_donnee": "episode_pollution",
                    "source": "api_epis_cleaned",
                    "polluant": polluant,
                    "import_date": datetime.now(),
                    
                    # Donn√©es de l'√©pisode
                    "aasqa": properties.get("aasqa"),
                    "date_maj": properties.get("date_maj"),
                    "lib_pol": properties.get("lib_pol"),
                    "lib_zone": properties.get("lib_zone"),
                    "etat": properties.get("etat"),
                    "date_ech": properties.get("date_ech"),
                    "date_dif": properties.get("date_dif"),
                    "code_zone": properties.get("code_zone"),
                    "code_pol": properties.get("code_pol"),
                    
                    # G√©om√©trie
                    "geometry_type": geometry.get("type"),
                    "coordinates": coordinates,
                    "latitude": coordinates[1] if len(coordinates) >= 2 else None,
                    "longitude": coordinates[0] if len(coordinates) >= 2 else None,
                    
                    # Document original
                    "feature_original": feature
                }
                
                documents.append(doc)
            
            # Ins√©rer les documents
            if documents:
                result = collection.insert_many(documents)
                imported_count = len(result.inserted_ids)
                print(f"   ‚úÖ {imported_count} √©pisodes import√©s dans {collection_name}")
                total_imported += imported_count
            
        except Exception as e:
            print(f"   ‚ùå Erreur traitement {filename}: {e}")
    
    print(f"\n‚úÖ IMPORT TERMIN√â : {total_imported} √©pisodes au total")
    
    # Lister les collections cr√©√©es
    collections = [col for col in db.list_collection_names() if col.startswith('episodes_')]
    print(f"üìä Collections cr√©√©es : {collections}")
    
    for col_name in collections:
        count = db[col_name].count_documents({})
        print(f"   - {col_name}: {count} documents")
    
    client.close()
    
    if total_imported > 0:
        print(f"\nüöÄ √âTAPE SUIVANTE :")
        print(f"   Maintenant ex√©cutez : python import_api&scrap_mongodb.py")
        print(f"   Pour r√©organiser en collections MOY_JOURNALIERE et EPIS_POLLUTION")

def group_episodes_to_epis():
    """Regrouper les 4 collections episodes_* dans EPIS_POLLUTION"""
    
    connection_string = os.getenv('MONGO_CONNECTION_STRING', 'mongodb://localhost:27017/')
    database_name = os.getenv('MONGO_DATABASE', 'pollution_app')
    
    print("üö® REGROUPEMENT DES √âPISODES DANS EPIS_POLLUTION")
    print("=" * 60)
    
    client = pymongo.MongoClient(connection_string)
    db = client[database_name]
    
    # Lister les collections existantes
    all_collections = db.list_collection_names()
    episode_collections = [col for col in all_collections if col.startswith('episodes_')]
    
    print(f"üìä Collections √©pisodes trouv√©es : {episode_collections}")
    
    if not episode_collections:
        print("‚ùå Aucune collection episodes_* trouv√©e")
        return
    
    # Cr√©er/vider la collection EPIS_POLLUTION
    epis_collection = db["EPIS_POLLUTION"]
    if "EPIS_POLLUTION" in all_collections:
        epis_collection.drop()
        print("üóëÔ∏è Ancienne collection EPIS_POLLUTION supprim√©e")
    
    # Regrouper toutes les collections episodes_*
    total_docs = 0
    
    for col_name in episode_collections:
        source_collection = db[col_name]
        docs = list(source_collection.find({}))
        
        print(f"üìÇ Migration {col_name} : {len(docs)} documents")
        
        # Ins√©rer les documents dans EPIS_POLLUTION
        if docs:
            try:
                result = epis_collection.insert_many(docs)
                inserted_count = len(result.inserted_ids)
                print(f"   ‚úÖ {inserted_count} documents migr√©s vers EPIS_POLLUTION")
                total_docs += inserted_count
            except Exception as e:
                print(f"   ‚ùå Erreur migration {col_name}: {e}")
                # Essayer un par un en cas d'erreur
                inserted = 0
                for doc in docs:
                    try:
                        epis_collection.insert_one(doc)
                        inserted += 1
                    except:
                        pass
                print(f"   ‚úÖ {inserted} documents migr√©s un par un")
                total_docs += inserted
    
    # Cr√©er des index pour optimiser les requ√™tes
    print("\nüîç Cr√©ation des index...")
    try:
        epis_collection.create_index([("polluant", 1)])
        epis_collection.create_index([("etat", 1)])
        epis_collection.create_index([("date_ech", 1)])
        epis_collection.create_index([("lib_zone", 1)])
        epis_collection.create_index([("aasqa", 1)])
        epis_collection.create_index([("latitude", 1), ("longitude", 1)])
        epis_collection.create_index([("type_donnee", 1)])
        print("‚úÖ Index cr√©√©s pour EPIS_POLLUTION")
    except Exception as e:
        print(f"‚ö†Ô∏è Erreur cr√©ation index: {e}")
    
    # Supprimer les anciennes collections episodes_*
    print(f"\nüóëÔ∏è Suppression des collections sources...")
    for col_name in episode_collections:
        try:
            db[col_name].drop()
            print(f"   üóëÔ∏è {col_name} supprim√©e")
        except Exception as e:
            print(f"   ‚ö†Ô∏è Erreur suppression {col_name}: {e}")
    
    # V√©rification finale
    final_count = epis_collection.count_documents({})
    print(f"\n‚úÖ REGROUPEMENT TERMIN√â")
    print(f"üìä Collection EPIS_POLLUTION : {final_count:,} documents")
    
    # Statistiques par polluant
    print(f"\nüìä R√©partition par polluant dans EPIS_POLLUTION :")
    try:
        pipeline = [
            {"$group": {"_id": "$polluant", "count": {"$sum": 1}}},
            {"$sort": {"_id": 1}}
        ]
        for result in epis_collection.aggregate(pipeline):
            if result['_id']:
                print(f"   - {result['_id']}: {result['count']:,} documents")
    except Exception as e:
        print(f"   ‚ö†Ô∏è Erreur statistiques: {e}")
    
    # Statistiques par √©tat
    print(f"\nüö® R√©partition par √©tat d'√©pisode :")
    try:
        pipeline = [
            {"$group": {"_id": "$etat", "count": {"$sum": 1}}},
            {"$sort": {"count": -1}}
        ]
        for result in epis_collection.aggregate(pipeline):
            if result['_id']:
                print(f"   - {result['_id']}: {result['count']:,} √©pisodes")
    except Exception as e:
        print(f"   ‚ö†Ô∏è Erreur statistiques par √©tat: {e}")
    
    client.close()
    print(f"\nüéâ Les √©pisodes sont maintenant regroup√©s dans EPIS_POLLUTION !")
    
    # Afficher l'√©tat final des collections
    print(f"\nüìã √âTAT FINAL DES COLLECTIONS :")
    client = pymongo.MongoClient(connection_string)
    db = client[database_name]
    
    final_collections = db.list_collection_names()
    for col in sorted(final_collections):
        count = db[col].count_documents({})
        print(f"   - {col}: {count:,} documents")
    
    client.close()

if __name__ == "__main__":
    import_episodes_to_mongo()
    group_episodes_to_epis()