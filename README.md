# Poll'Air

Syst√®me de collecte, traitement et mise √† disposition de donn√©es de pollution atmosph√©rique en France et territoires d'Outre-Mer

---

## Vue d'ensemble du projet-BLOC 1

Poll'Air est un projet complet de data engineering qui impl√©mente les comp√©tences du bloc "D√©velopper la mise √† disposition technique des donn√©es collect√©es pour un projet d'intelligence artificielle". Le syst√®me traite 814,242 mesures de pollution atmosph√©rique provenant de sources multiples pour constituer une base de donn√©es coh√©rente et exploitable, avec pour finalit√© de g√©n√©rer des recommandations personnalis√©es selon diff√©rents types de profils utilisateur (sportifs, personnes sensibles, parents, seniors).

### Architecture technique g√©n√©rale

```
Sources de donn√©es ‚Üí Pipeline ETL ‚Üí Base de donn√©es ‚Üí API REST ‚Üí Interface utilisateur
```

## üì° Comp√©tence C1 : Programmer la collecte de donn√©es depuis plusieurs sources

### Sources de donn√©es int√©gr√©es

#### 1. Extraction API REST (automatis√©e)
**Localisation** : `scripts/data recovery/extract_from_api.py`
- API officielles AASQA (Associations Agr√©√©es de Surveillance de la Qualit√© de l'Air)
- Format JSON structur√©
- Donn√©es en temps r√©el et historiques
- Gestion d'erreurs et retry automatique

#### 2. Web scraping (automatis√©)
**Localisation** : `scripts/data recovery/scraping_geodair.py`
- Portail national GeodAir
- Parsing HTML avec BeautifulSoup4
- Extraction moyennes journali√®res par polluant
- Gestion sessions et headers

#### 3. Fichiers CSV institutionnels
**Localisation** : `data/file-indices_qualite_air-*/`
- Donn√©es consolid√©es AASQA par r√©gion
- Import automatis√© avec pandas
- Validation format et structure

### Technologies utilis√©es pour la collecte
- **Python requests** : Appels API REST
- **Selenium/webdriver** : Web scraping
- **pandas** : Lecture fichiers structur√©s
- **urllib** : Gestion URLs et sessions

## üóÑÔ∏è Comp√©tence C2 : D√©velopper des requ√™tes pour l'extraction depuis syst√®me de gestion de base de donn√©es

### Base de donn√©es PostgreSQL

#### Architecture relationnelle
**Localisation** : `scripts/sql/`

```sql
-- Table principale (814,242 enregistrements)
qualite_air (
    id SERIAL PRIMARY KEY,
    code_insee VARCHAR(5),
    date_mesure DATE,
    polluant_id INTEGER REFERENCES polluants(id),
    valeur NUMERIC(8,2),
    niveau_qualite VARCHAR(20),
    station VARCHAR(100),
    aasqa_code VARCHAR(10)
);

-- Tables de r√©f√©rence
polluants (id, code_polluant, nom, unite, seuil_info, seuil_alerte)
communes (code_insee, nom_commune, region, departement)  
episodes_pollution (date_episode, aasqa_code, niveau)
```

#### Requ√™tes d'extraction optimis√©es
**Localisation** : `scripts/sql/check_tables.py`

```sql
-- Extraction donn√©es par r√©gion et p√©riode
SELECT 
    qa.date_mesure,
    c.nom_commune,
    p.code_polluant,
    qa.valeur,
    qa.niveau_qualite
FROM qualite_air qa
JOIN communes c ON qa.code_insee = c.code_insee
JOIN polluants p ON qa.polluant_id = p.id
WHERE qa.date_mesure BETWEEN %s AND %s
    AND c.region = %s
ORDER BY qa.date_mesure DESC;

-- Agr√©gations temporelles
SELECT 
    EXTRACT(MONTH FROM date_mesure) as mois,
    AVG(valeur) as moyenne_mensuelle,
    MAX(valeur) as pic_pollution
FROM qualite_air qa
JOIN polluants p ON qa.polluant_id = p.id
WHERE p.code_polluant = 'NO2'
GROUP BY EXTRACT(MONTH FROM date_mesure);
```

#### Index de performance
```sql
CREATE INDEX idx_qualite_air_date_commune ON qualite_air (date_mesure, code_insee);
CREATE INDEX idx_qualite_air_polluant ON qualite_air (polluant_id, valeur);
```

## üßπ Comp√©tence C3 : D√©velopper des r√®gles d'agr√©gation de donn√©es et suppression des entr√©es corrompues

### Pipeline de nettoyage et agr√©gation
**Localisation** : `scripts/data cleaning+standardization/`

#### Suppression des entr√©es corrompues
**Fichier** : `clean_api.py`, `clean_csv.py`, `clean_scraping.py`

```python
def nettoyer_donnees_pollution(df):
    # Suppression valeurs aberrantes
    df = df[df['valeur'] >= 0]  # Pas de valeurs n√©gatives
    df = df[df['valeur'] <= 1000]  # Seuil maximum r√©aliste
    
    # Suppression doublons
    df = df.drop_duplicates(subset=['code_insee', 'date_mesure', 'polluant'])
    
    # Validation codes g√©ographiques
    codes_insee_valides = charger_referentiel_insee()
    df = df[df['code_insee'].isin(codes_insee_valides)]
    
    # Validation dates coh√©rentes
    df = df[df['date_mesure'] <= datetime.now().date()]
    
    return df
```

#### R√®gles d'agr√©gation m√©tier
```python
def agreger_donnees_journalieres(df):
    # Agr√©gation par commune/polluant/jour
    aggregations = {
        'valeur': ['mean', 'max', 'min', 'count'],
        'niveau_qualite': 'first'
    }
    
    df_agg = df.groupby(['code_insee', 'date_mesure', 'polluant']).agg(aggregations)
    
    # Calcul indices qualit√© personnalis√©s
    df_agg['indice_composite'] = calculer_indice_composite(df_agg)
    
    return df_agg
```

#### Standardisation et normalisation
```python
def standardiser_formats(df):
    # Normalisation unit√©s (tout en ¬µg/m¬≥)
    df['valeur'] = df.apply(convertir_unite_standard, axis=1)
    
    # Standardisation codes polluants
    mapping_polluants = {'NO‚ÇÇ': 'NO2', 'PM2.5': 'PM25'}
    df['code_polluant'] = df['code_polluant'].replace(mapping_polluants)
    
    # Format dates ISO
    df['date_mesure'] = pd.to_datetime(df['date_mesure']).dt.date
    
    return df
```

### R√©sultats du nettoyage
- **814,242 mesures** valid√©es sur 850,000 initiales
- **99.7%** de taux de validation
- **D√©duplication** : 15,000 doublons supprim√©s
- **Valeurs aberrantes** : 1,200 entr√©es √©limin√©es

## üîí Comp√©tence C4 : Cr√©er une base de donn√©es respectant le RGPD

### Conformit√© RGPD impl√©ment√©e
**Localisation** : `docs/rgpd/`

#### Mod√®les de donn√©es respectueux
**Fichier** : `scripts/sql/create_table_profils-seuils-recos.py`

```sql
-- Table profils avec minimisation des donn√©es
CREATE TABLE profils_utilisateurs (
    id SERIAL PRIMARY KEY,
    email VARCHAR(255) UNIQUE NOT NULL,
    type_profil VARCHAR(50), -- 'sportif', 'sensible', etc.
    commune_residence VARCHAR(5), -- Code INSEE uniquement
    date_creation TIMESTAMP DEFAULT NOW(),
    consentement_collecte BOOLEAN DEFAULT FALSE,
    date_derniere_connexion TIMESTAMP
);

-- Pas de stockage nom/pr√©nom/adresse compl√®te
-- Donn√©es pseudonymis√©es avec ID technique
```

#### Proc√©dures RGPD
**Fichier** : `docs/procedures_conformite_rgpd.md`

1. **Droit √† l'effacement** : Proc√©dure suppression profil complet
2. **Portabilit√©** : Export donn√©es utilisateur format JSON
3. **Rectification** : Interface modification profil
4. **Limitation traitement** : D√©sactivation profil sans suppression

#### Registre des traitements
**Fichier** : `docs/registre_traitements_rgpd.md`
- Finalit√© : Recommandations qualit√© air personnalis√©es
- Base l√©gale : Consentement utilisateur
- Dur√©e conservation : 24 mois maximum
- Destinataires : Utilisateur uniquement

## üöÄ Comp√©tence C5 : D√©velopper une API REST pour l'exploitation des donn√©es

### Architecture API FastAPI
**Localisation** : `scripts/api/`

#### Points d'entr√©e REST
**Fichier** : `main.py`

```python
from fastapi import FastAPI, Depends, HTTPException
from routers import air_quality, auth, profils

app = FastAPI(
    title="Poll'Air API",
    description="API de donn√©es pollution atmosph√©rique",
    version="1.0.0"
)

app.include_router(air_quality.router, prefix="/api")
app.include_router(auth.router, prefix="/auth") 
app.include_router(profils.router, prefix="/profils")
```

#### Endpoints principaux
**Fichier** : `routers/air_quality.py`

```python
@router.get("/qualite-air")
def get_qualite_air(
    code_insee: Optional[str] = None,
    date_debut: Optional[date] = None,
    date_fin: Optional[date] = None,
    polluant: Optional[str] = None,
    limit: int = 50
):
    """R√©cup√©ration donn√©es qualit√© air avec filtres"""
    
@router.get("/episodes-pollution")  
def get_episodes_pollution(
    aasqa: Optional[str] = None,
    date_debut: Optional[date] = None,
    limit: int = 20
):
    """Consultation √©pisodes de pollution"""

@router.get("/indices-aasqa")
def get_indices_aasqa(format: Optional[str] = "json"):
    """Export indices qualit√© air (JSON/CSV)"""
```

#### Authentification JWT
**Fichier** : `routers/auth.py`

```python
@router.post("/login")
def login(credentials: UserCredentials):
    """Authentification utilisateur avec JWT"""
    
def get_current_user(token: str = Depends(oauth2_scheme)):
    """Validation token JWT pour endpoints prot√©g√©s"""
```

#### Architecture REST respect√©e
- **GET** : Consultation donn√©es (stateless)
- **POST** : Cr√©ation profils utilisateur
- **PUT** : Modification profils
- **DELETE** : Suppression profils (RGPD)
- **Content-Type** : application/json
- **Codes HTTP** : 200, 201, 400, 401, 404, 500

#### S√©curit√© API
**Fichier** : `security/rate_limiting.py`
- Rate limiting par IP
- Validation entr√©es (Pydantic)
- Authentification JWT
- Logging s√©curis√©

### Documentation automatique
- **Swagger UI** : `/docs`
- **ReDoc** : `/redoc`
- **OpenAPI Schema** : `/openapi.json`

## Architecture technique globale

### Stack technologique
- **Backend** : Python 3.9, FastAPI
- **Base de donn√©es** : PostgreSQL 12+, MongoDB 4.4
- **ETL** : pandas, SQLAlchemy, psycopg2
- **Web scraping** : requests, Selenium
- **API** : FastAPI, Pydantic, python-jose
- **S√©curit√©** : bcrypt, slowapi

### Performance et optimisation
- **Index database** : Requ√™tes < 100ms
- **Pagination** : Limitation r√©sultats par d√©faut
- **Cache** : Donn√©es fr√©quentes en m√©moire
- **Connection pooling** : Gestion connexions DB

### Structure des fichiers de donn√©es

#### Donn√©es brutes
```
data/
‚îú‚îÄ‚îÄ api-epis_pollution-01-06-2024_01-06-2025/    # Donn√©es API brutes
‚îú‚îÄ‚îÄ file-indices_qualite_air-01-06-2024_01-06-2025/  # CSV institutionnels
‚îî‚îÄ‚îÄ scraping-moy_journaliere/                    # Donn√©es scraping brutes
```

#### Donn√©es nettoy√©es
```
data/
‚îú‚îÄ‚îÄ api-epis_pollution_cleaned/                  # API nettoy√©es
‚îú‚îÄ‚îÄ file-indices_nettoyes/                       # CSV standardis√©s
‚îî‚îÄ‚îÄ scraping-moy_journaliere_cleaned/            # Scraping valid√©es
```

## Installation et utilisation

### Pr√©requis
- Python 3.8+
- PostgreSQL 12+
- 4 GB RAM minimum

### Installation
```bash
# D√©pendances
pip install -r requirements.txt

# Configuration
cp .env.example .env
# √âditer .env avec param√®tres database

# Import donn√©es
cd scripts/sql
python initialisation_complete.py
```

### Lancement API
```bash
cd scripts/api
uvicorn main:app --reload --port 8000
# API disponible sur http://localhost:8000
```

## Livrables et comp√©tences d√©montr√©es

### üì° C1 - Collecte multi-sources
- Scripts d'extraction API, scraping, CSV
- Gestion erreurs et formats h√©t√©rog√®nes
- 814,242 mesures collect√©es et valid√©es

### üóÑÔ∏è C2 - Requ√™tes base de donn√©es
- Mod√®le relationnel PostgreSQL optimis√©
- Requ√™tes complexes avec jointures et agr√©gations
- Index de performance pour analytics

### üßπ C3 - Nettoyage et agr√©gation
- Pipeline ETL avec r√®gles m√©tier
- Suppression 35,000+ entr√©es corrompues
- Standardisation formats et r√©f√©rentiels

### üîí C4 - Base RGPD
- Minimisation donn√©es personnelles
- Proc√©dures droits utilisateurs
- Registre des traitements conforme

### üöÄ C5 - API REST
- Endpoints FastAPI document√©s
- Authentification JWT
- Respect principes REST et s√©curit√©

---

Poll'Air - D√©monstration compl√®te des comp√©tences de mise √† disposition technique des donn√©es pour l'intelligence artificielle